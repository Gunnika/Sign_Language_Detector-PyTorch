{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "import os\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from barbar import Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = np.array(glob('asl-alphabet/asl_alphabet_train/asl_alphabet_train/*/*'))\n",
    "test_data_raw = np.array(glob('asl-alphabet/asl_alphabet_test/asl_alphabet_test/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 87000 total train images.\n",
      "There are 28 total test images.\n"
     ]
    }
   ],
   "source": [
    "print('There are %d total train images.' % len(train_data_raw))\n",
    "print('There are %d total test images.' % len(test_data_raw)) #No test image for delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 69600 total train images.\n",
      "There are 17400 total validation images.\n",
      "There are 28 total test images.\n"
     ]
    }
   ],
   "source": [
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "train_transform = transforms.Compose([ transforms.Grayscale(num_output_channels=1),\n",
    "                                transforms.Resize(size=(50,50)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "valid_transform = transforms.Compose([ transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.Resize(50),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.5], [0.5])])\n",
    "test_transform = transforms.Compose([ transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.Resize(size=(50,50)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "train_data = datasets.ImageFolder(root = 'asl-alphabet/asl_alphabet_train/asl_alphabet_train', transform=train_transform)\n",
    "test_data = datasets.ImageFolder(root='asl-alphabet/asl_alphabet_test', transform=test_transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total train images.' % len(indices[split:]))\n",
    "print('There are %d total validation images.' % len(indices[:split]))\n",
    "print('There are %d total test images.' % len(test_data))\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=20,sampler=train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(train_data, batch_size=20,sampler=valid_sampler)\n",
    "testloader = torch.utils.data.DataLoader(test_data,batch_size=20, shuffle=False)\n",
    "\n",
    "\n",
    "loaders = dict(train=trainloader,\n",
    "                       valid = validloader,\n",
    "                       test=testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,3)\n",
    "        self.conv2 = nn.Conv2d(10,20,3)\n",
    "        self.conv3 = nn.Conv2d(20,30,3)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout2d(0.2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2430, 270)\n",
    "        self.fc2 = nn.Linear(270,29)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.softmax(F.relu(self.fc2(x)))\n",
    "        return(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    model = Network().cuda()\n",
    "else:\n",
    "    model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 48, 48]             100\n",
      "         MaxPool2d-2           [-1, 10, 24, 24]               0\n",
      "            Conv2d-3           [-1, 20, 22, 22]           1,820\n",
      "         MaxPool2d-4           [-1, 20, 11, 11]               0\n",
      "            Conv2d-5             [-1, 30, 9, 9]           5,430\n",
      "         Dropout2d-6             [-1, 30, 9, 9]               0\n",
      "            Linear-7                  [-1, 270]         656,370\n",
      "            Linear-8                   [-1, 29]           7,859\n",
      "        LogSoftmax-9                   [-1, 29]               0\n",
      "================================================================\n",
      "Total params: 671,579\n",
      "Trainable params: 671,579\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 2.56\n",
      "Estimated Total Size (MB): 2.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1,dim,dim)) #takes the model and the input tensor shape, displays the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), learning_rate, momentum=0.007)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        print('Epoch: {} '.format(\n",
    "        epoch\n",
    "        ))\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(Bar(loaders['train'])):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1/(batch_idx +1))*(loss.data - train_loss))\n",
    "            \n",
    "#             if batch_idx % 1000 == 0:\n",
    "#                 print('Epoch %d, Batch %d loss: %.6f' %(epoch, batch_idx + 1, train_loss))\n",
    "            \n",
    "#         print('Training Loss: {:.6f} '.format(\n",
    "#         train_loss\n",
    "#         ))\n",
    "                \n",
    "    # return trained model\n",
    "#     return model\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "\n",
    "\n",
    "#       print training/validation statistics \n",
    "        print('  Training Loss: {:.6f} \\tValidation Loss: {:.6f}'.format( \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      "69600/87000: [===============================>] - ETA 0.3ssss  Training Loss: 3.366137 \tValidation Loss: 3.364683\n",
      "Validation loss decreased (inf --> 3.364683).  Saving model ...\n",
      "Epoch: 2 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 3.363264 \tValidation Loss: 3.360312\n",
      "Validation loss decreased (3.364683 --> 3.360312).  Saving model ...\n",
      "Epoch: 3 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 3.356340 \tValidation Loss: 3.347574\n",
      "Validation loss decreased (3.360312 --> 3.347574).  Saving model ...\n",
      "Epoch: 4 \n",
      "69600/87000: [===============================>] - ETA 0.1sss  Training Loss: 3.332255 \tValidation Loss: 3.297224\n",
      "Validation loss decreased (3.347574 --> 3.297224).  Saving model ...\n",
      "Epoch: 5 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 3.262462 \tValidation Loss: 3.185503\n",
      "Validation loss decreased (3.297224 --> 3.185503).  Saving model ...\n",
      "Epoch: 6 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 3.114547 \tValidation Loss: 2.977447\n",
      "Validation loss decreased (3.185503 --> 2.977447).  Saving model ...\n",
      "Epoch: 7 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 2.901483 \tValidation Loss: 2.736497\n",
      "Validation loss decreased (2.977447 --> 2.736497).  Saving model ...\n",
      "Epoch: 8 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 2.658058 \tValidation Loss: 2.475004\n",
      "Validation loss decreased (2.736497 --> 2.475004).  Saving model ...\n",
      "Epoch: 9 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 2.386621 \tValidation Loss: 2.195658\n",
      "Validation loss decreased (2.475004 --> 2.195658).  Saving model ...\n",
      "Epoch: 10 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 2.154064 \tValidation Loss: 2.008970\n",
      "Validation loss decreased (2.195658 --> 2.008970).  Saving model ...\n",
      "Epoch: 11 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.966985 \tValidation Loss: 1.817686\n",
      "Validation loss decreased (2.008970 --> 1.817686).  Saving model ...\n",
      "Epoch: 12 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.761626 \tValidation Loss: 1.663208\n",
      "Validation loss decreased (1.817686 --> 1.663208).  Saving model ...\n",
      "Epoch: 13 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.574108 \tValidation Loss: 1.444611\n",
      "Validation loss decreased (1.663208 --> 1.444611).  Saving model ...\n",
      "Epoch: 14 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.414845 \tValidation Loss: 1.302124\n",
      "Validation loss decreased (1.444611 --> 1.302124).  Saving model ...\n",
      "Epoch: 15 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.287115 \tValidation Loss: 1.190799\n",
      "Validation loss decreased (1.302124 --> 1.190799).  Saving model ...\n",
      "Epoch: 16 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.185542 \tValidation Loss: 1.071570\n",
      "Validation loss decreased (1.190799 --> 1.071570).  Saving model ...\n",
      "Epoch: 17 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.102929 \tValidation Loss: 1.005827\n",
      "Validation loss decreased (1.071570 --> 1.005827).  Saving model ...\n",
      "Epoch: 18 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 1.028848 \tValidation Loss: 0.937807\n",
      "Validation loss decreased (1.005827 --> 0.937807).  Saving model ...\n",
      "Epoch: 19 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.971343 \tValidation Loss: 0.885110\n",
      "Validation loss decreased (0.937807 --> 0.885110).  Saving model ...\n",
      "Epoch: 20 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.897641 \tValidation Loss: 0.810374\n",
      "Validation loss decreased (0.885110 --> 0.810374).  Saving model ...\n",
      "Epoch: 21 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.810034 \tValidation Loss: 0.753576\n",
      "Validation loss decreased (0.810374 --> 0.753576).  Saving model ...\n",
      "Epoch: 22 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.774324 \tValidation Loss: 0.695490\n",
      "Validation loss decreased (0.753576 --> 0.695490).  Saving model ...\n",
      "Epoch: 23 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.737826 \tValidation Loss: 0.695760\n",
      "Epoch: 24 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.709704 \tValidation Loss: 0.651243\n",
      "Validation loss decreased (0.695490 --> 0.651243).  Saving model ...\n",
      "Epoch: 25 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.683725 \tValidation Loss: 0.636787\n",
      "Validation loss decreased (0.651243 --> 0.636787).  Saving model ...\n",
      "Epoch: 26 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.664589 \tValidation Loss: 0.594259\n",
      "Validation loss decreased (0.636787 --> 0.594259).  Saving model ...\n",
      "Epoch: 27 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.639954 \tValidation Loss: 0.588745\n",
      "Validation loss decreased (0.594259 --> 0.588745).  Saving model ...\n",
      "Epoch: 28 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.625685 \tValidation Loss: 0.605705\n",
      "Epoch: 29 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.603342 \tValidation Loss: 0.556880\n",
      "Validation loss decreased (0.588745 --> 0.556880).  Saving model ...\n",
      "Epoch: 30 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.590830 \tValidation Loss: 0.543604\n",
      "Validation loss decreased (0.556880 --> 0.543604).  Saving model ...\n",
      "Epoch: 31 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.578789 \tValidation Loss: 0.567975\n",
      "Epoch: 32 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.563089 \tValidation Loss: 0.525778\n",
      "Validation loss decreased (0.543604 --> 0.525778).  Saving model ...\n",
      "Epoch: 33 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.554440 \tValidation Loss: 0.530841\n",
      "Epoch: 34 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.545384 \tValidation Loss: 0.519735\n",
      "Validation loss decreased (0.525778 --> 0.519735).  Saving model ...\n",
      "Epoch: 35 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.534530 \tValidation Loss: 0.495860\n",
      "Validation loss decreased (0.519735 --> 0.495860).  Saving model ...\n",
      "Epoch: 36 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.531550 \tValidation Loss: 0.497954\n",
      "Epoch: 37 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.518948 \tValidation Loss: 0.498872\n",
      "Epoch: 38 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.511622 \tValidation Loss: 0.500408\n",
      "Epoch: 39 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.501187 \tValidation Loss: 0.483960\n",
      "Validation loss decreased (0.495860 --> 0.483960).  Saving model ...\n",
      "Epoch: 40 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.488422 \tValidation Loss: 0.497400\n",
      "Epoch: 41 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.485853 \tValidation Loss: 0.482739\n",
      "Validation loss decreased (0.483960 --> 0.482739).  Saving model ...\n",
      "Epoch: 42 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.480522 \tValidation Loss: 0.483336\n",
      "Epoch: 43 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.471687 \tValidation Loss: 0.435371\n",
      "Validation loss decreased (0.482739 --> 0.435371).  Saving model ...\n",
      "Epoch: 44 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.461804 \tValidation Loss: 0.437109\n",
      "Epoch: 45 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.454012 \tValidation Loss: 0.427711\n",
      "Validation loss decreased (0.435371 --> 0.427711).  Saving model ...\n",
      "Epoch: 46 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.448045 \tValidation Loss: 0.425354\n",
      "Validation loss decreased (0.427711 --> 0.425354).  Saving model ...\n",
      "Epoch: 47 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.443801 \tValidation Loss: 0.412083\n",
      "Validation loss decreased (0.425354 --> 0.412083).  Saving model ...\n",
      "Epoch: 48 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.438372 \tValidation Loss: 0.413837\n",
      "Epoch: 49 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.435556 \tValidation Loss: 0.420999\n",
      "Epoch: 50 \n",
      "69600/87000: [===============================>] - ETA 0.0sss  Training Loss: 0.421619 \tValidation Loss: 0.391539\n",
      "Validation loss decreased (0.412083 --> 0.391539).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# model_scratch = train(epochs, loaders, model, optimizer, criterion, use_cuda, 'saved_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy \n",
    "if use_cuda:\n",
    "    infer_model = Network().cuda()\n",
    "else:\n",
    "    infer_model = Network()\n",
    "infer_model.load_state_dict(torch.load('saved_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = {\n",
    "    0:'A',\n",
    "    1:'B',\n",
    "    2:'C',\n",
    "    3:'D',\n",
    "    4:'E',\n",
    "    5:'F',\n",
    "    6:'G',\n",
    "    7:'H',\n",
    "    8:'I',\n",
    "    9:'J',\n",
    "    10:'K',\n",
    "    11:'L',\n",
    "    12:'M',\n",
    "    13:'N',\n",
    "    14:'O',\n",
    "    15:'P',\n",
    "    16:'Q',\n",
    "    17:'R',\n",
    "    18:'S',\n",
    "    19:'T',\n",
    "    20:'U',\n",
    "    21:'V',\n",
    "    22:'W',\n",
    "    23:'X',\n",
    "    24:'Y',\n",
    "    25:'Z',\n",
    "    26:'del',\n",
    "    27:'nothing',\n",
    "    28:'space'\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: A\n",
      "correct 0.0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: U\n",
      "correct 20.0\n",
      "\n",
      "Test Loss: 28.085033\n",
      "\n",
      "\n",
      "Test Accuracy: 71% (20/28)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "    \n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        _, preds_tensor = torch.max(output, 1)\n",
    "\n",
    "        predimg = np.squeeze(preds_tensor.numpy()[0]) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy()[0])\n",
    "        print(batch_idx)\n",
    "        np_img = data.cpu().numpy()[0][0]\n",
    "        plt.imshow(np_img, cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Prediction: {}\".format(dict_labels[predimg]))\n",
    "        \n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(predimg)\n",
    "        print(\"correct\",correct)\n",
    "#         correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "    \n",
    "            \n",
    "    print('\\nTest Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * float(correct / total), correct, total))\n",
    "\n",
    "# call test function    \n",
    "test(loaders, infer_model, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-dc1901dec905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-dc1901dec905>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: tensor(0)"
     ]
    }
   ],
   "source": [
    "# def imshow(inp, title=None): \n",
    "#     \"\"\"Imshow for Tensor.\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(loaders['test']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[dict_labels[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5686, -0.4431, -0.4745,  ..., -0.4667, -0.4431, -0.4745],\n",
      "          [-0.3490, -0.0196, -0.0039,  ..., -0.1922, -0.1843, -0.2863],\n",
      "          [-0.4980, -0.2941, -0.2706,  ..., -0.3412, -0.2784, -0.3412],\n",
      "          ...,\n",
      "          [-0.5686, -0.4118, -0.3882,  ...,  0.0118,  0.0118, -0.1216],\n",
      "          [-0.5608, -0.3961, -0.3804,  ...,  0.0275,  0.0275, -0.1216],\n",
      "          [-0.6000, -0.4588, -0.4510,  ..., -0.1059, -0.1137, -0.2235]]],\n",
      "\n",
      "\n",
      "        [[[-0.7020, -0.6549, -0.6549,  ..., -0.0824, -0.0588, -0.1922],\n",
      "          [-0.6471, -0.5529, -0.5608,  ...,  0.4353,  0.4510,  0.2314],\n",
      "          [-0.5686, -0.4431, -0.4588,  ...,  0.3176,  0.3490,  0.1686],\n",
      "          ...,\n",
      "          [-0.2706,  0.1137,  0.1216,  ...,  0.1451,  0.1294, -0.0353],\n",
      "          [-0.2627,  0.1216,  0.1216,  ...,  0.1373,  0.1216, -0.0510],\n",
      "          [-0.3569, -0.0431, -0.0353,  ..., -0.0275, -0.0510, -0.1922]]],\n",
      "\n",
      "\n",
      "        [[[-0.6941, -0.6314, -0.6314,  ..., -0.0510, -0.0275, -0.1686],\n",
      "          [-0.6157, -0.5059, -0.5059,  ...,  0.4902,  0.4980,  0.2784],\n",
      "          [-0.5373, -0.3882, -0.3961,  ...,  0.3882,  0.4275,  0.2392],\n",
      "          ...,\n",
      "          [-0.2392,  0.1608,  0.1686,  ...,  0.2078,  0.1922,  0.0118],\n",
      "          [-0.2392,  0.1608,  0.1686,  ...,  0.2000,  0.1843,  0.0039],\n",
      "          [-0.3333, -0.0039, -0.0039,  ...,  0.0196,  0.0118, -0.1451]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5686, -0.4196, -0.4275,  ..., -0.6000, -0.5843, -0.6000],\n",
      "          [-0.3020,  0.0275, -0.0039,  ..., -0.4902, -0.4980, -0.5529],\n",
      "          [-0.1373,  0.3333,  0.3333,  ..., -0.4275, -0.4275, -0.4980],\n",
      "          ...,\n",
      "          [-0.4902, -0.4039, -0.5608,  ..., -0.1608, -0.1529, -0.2627],\n",
      "          [-0.7569, -0.7569, -0.7490,  ..., -0.1608, -0.1608, -0.2627],\n",
      "          [-0.7804, -0.8196, -0.8353,  ..., -0.2627, -0.2627, -0.3569]]],\n",
      "\n",
      "\n",
      "        [[[-0.5529, -0.3961, -0.3961,  ..., -0.6549, -0.6078, -0.6157],\n",
      "          [-0.2471,  0.0980,  0.0510,  ..., -0.5686, -0.5765, -0.6235],\n",
      "          [-0.0510,  0.4745,  0.4588,  ..., -0.4510, -0.4667, -0.5373],\n",
      "          ...,\n",
      "          [-0.4353, -0.3412, -0.5059,  ..., -0.0667, -0.0824, -0.2078],\n",
      "          [-0.7333, -0.7176, -0.7255,  ..., -0.0667, -0.0745, -0.2078],\n",
      "          [-0.7647, -0.8039, -0.8275,  ..., -0.2000, -0.2078, -0.3098]]],\n",
      "\n",
      "\n",
      "        [[[-0.5529, -0.4039, -0.4118,  ..., -0.5843, -0.5529, -0.5765],\n",
      "          [-0.2863,  0.0510,  0.0118,  ..., -0.4588, -0.4588, -0.5137],\n",
      "          [-0.1216,  0.3569,  0.3412,  ..., -0.3804, -0.3961, -0.4667],\n",
      "          ...,\n",
      "          [-0.4588, -0.3647, -0.5294,  ..., -0.0824, -0.0902, -0.2157],\n",
      "          [-0.7490, -0.7412, -0.7412,  ..., -0.0824, -0.0902, -0.2157],\n",
      "          [-0.7725, -0.8039, -0.8196,  ..., -0.2078, -0.2157, -0.3176]]]]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[[[-0.5608, -0.4196, -0.4196,  ..., -0.5922, -0.5686, -0.5922],\n",
      "          [-0.3020,  0.0275, -0.0118,  ..., -0.4824, -0.4824, -0.5373],\n",
      "          [-0.1216,  0.3569,  0.3412,  ..., -0.4118, -0.4196, -0.4902],\n",
      "          ...,\n",
      "          [-0.5765, -0.4902, -0.5529,  ..., -0.1294, -0.1373, -0.2471],\n",
      "          [-0.6471, -0.4902, -0.5373,  ..., -0.1294, -0.1294, -0.2471],\n",
      "          [-0.7255, -0.5686, -0.5686,  ..., -0.2471, -0.2471, -0.3412]]],\n",
      "\n",
      "\n",
      "        [[[-0.5765, -0.4431, -0.4431,  ..., -0.6078, -0.5765, -0.6000],\n",
      "          [-0.3255, -0.0196, -0.0510,  ..., -0.4980, -0.4980, -0.5529],\n",
      "          [-0.1608,  0.2941,  0.2784,  ..., -0.4275, -0.4353, -0.4980],\n",
      "          ...,\n",
      "          [-0.4980, -0.4196, -0.5608,  ..., -0.1765, -0.1765, -0.2863],\n",
      "          [-0.7647, -0.7569, -0.7333,  ..., -0.1765, -0.1765, -0.2863],\n",
      "          [-0.7882, -0.8196, -0.8353,  ..., -0.2863, -0.2863, -0.3725]]],\n",
      "\n",
      "\n",
      "        [[[-0.5843, -0.4353, -0.4431,  ..., -0.6157, -0.5843, -0.6000],\n",
      "          [-0.3255, -0.0118, -0.0588,  ..., -0.5059, -0.4980, -0.5608],\n",
      "          [-0.1608,  0.2784,  0.2627,  ..., -0.4353, -0.4431, -0.5059],\n",
      "          ...,\n",
      "          [-0.5137, -0.4196, -0.5686,  ..., -0.1686, -0.1608, -0.2706],\n",
      "          [-0.7647, -0.7647, -0.7647,  ..., -0.1608, -0.1686, -0.2784],\n",
      "          [-0.7804, -0.8275, -0.8431,  ..., -0.2706, -0.2784, -0.3725]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5765, -0.4353, -0.4353,  ..., -0.5922, -0.5686, -0.6000],\n",
      "          [-0.3412, -0.0275, -0.0588,  ..., -0.4980, -0.5059, -0.5608],\n",
      "          [-0.1608,  0.2784,  0.2627,  ..., -0.4275, -0.4431, -0.5059],\n",
      "          ...,\n",
      "          [-0.4588, -0.3569, -0.5294,  ..., -0.1765, -0.1686, -0.2784],\n",
      "          [-0.7412, -0.7490, -0.7490,  ..., -0.1765, -0.1686, -0.2706],\n",
      "          [-0.7725, -0.8118, -0.8275,  ..., -0.2863, -0.2784, -0.3647]]],\n",
      "\n",
      "\n",
      "        [[[-0.7176, -0.6863, -0.6784,  ..., -0.1216, -0.1137, -0.2392],\n",
      "          [-0.6706, -0.6078, -0.6157,  ...,  0.3333,  0.3412,  0.1451],\n",
      "          [-0.6000, -0.4980, -0.5137,  ...,  0.2471,  0.2863,  0.1137],\n",
      "          ...,\n",
      "          [-0.3098,  0.0353,  0.0353,  ...,  0.0902,  0.0745, -0.0824],\n",
      "          [-0.3098,  0.0353,  0.0431,  ...,  0.0824,  0.0667, -0.0902],\n",
      "          [-0.3961, -0.1059, -0.1059,  ..., -0.0745, -0.0902, -0.2235]]],\n",
      "\n",
      "\n",
      "        [[[-0.5922, -0.4510, -0.4510,  ..., -0.6078, -0.5843, -0.6000],\n",
      "          [-0.3490, -0.0588, -0.0980,  ..., -0.5216, -0.5294, -0.5765],\n",
      "          [-0.1765,  0.2549,  0.2314,  ..., -0.4588, -0.4745, -0.5294],\n",
      "          ...,\n",
      "          [-0.4588, -0.3569, -0.5216,  ..., -0.1765, -0.1765, -0.2863],\n",
      "          [-0.7490, -0.7412, -0.7490,  ..., -0.1765, -0.1765, -0.2863],\n",
      "          [-0.7804, -0.8118, -0.8353,  ..., -0.2863, -0.2863, -0.3725]]]]) tensor([0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "    print(data,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    # load the image and return the predicted breed\n",
    "    img = Image.open(img_path)\n",
    "#     img = Image.open(img_path).convert('L')\n",
    "    transformations = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                            transforms.Resize(size=50),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.5],[0.5])])\n",
    "    image_tensor = transformations(img)[:3,:,:].unsqueeze(0)\n",
    "#     image_tensor = transformations(img)[:3,:,:].unsqueeze(0)\n",
    "\n",
    "    # move model inputs to cuda, if GPU available\n",
    "    if use_cuda:\n",
    "        image_tensor = image_tensor.cuda()\n",
    "\n",
    "    # get sample outputs\n",
    "    output = infer_model(image_tensor)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "\n",
    "    pred = np.squeeze(preds_tensor.numpy()[0]) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy()[0])\n",
    "\n",
    "    return dict_labels[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "# plt.plot(loss_log[2:])\n",
    "# plt.plot(acc_log)\n",
    "# plt.plot(np.ones(len(acc_log)), linestyle='dashed')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict('Inference_Images/c.jpg')\n",
    "lab = 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: nothing\n",
      "Actual Label: c\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction: {}\".format(prediction))\n",
    "print(\"Actual Label: {}\".format(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels = cv2.imread('./c.jpg').reshape(28, 28)\n",
    "# plt.subplot(223)\n",
    "# sns.heatmap(data=pixels)\n",
    "# lab = 'c'\n",
    "# test_sample = torch.FloatTensor([pixels.reshape(1, 28, 28).tolist()])\n",
    "# pred = model(Variable(input_img))\n",
    "# print(\"Prediction: {}\".format(alph[torch.max(net_out_sample.data, 1)[1].numpy()[0]]))\n",
    "# print(\"Actual Label: {}\".format(lab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(img):\n",
    "    # load the image and return the predicted breed\n",
    "#     img = Image.open(img_path).convert('L')\n",
    "    img = Image.fromarray(img)\n",
    "    transformations = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
    "                                            transforms.Resize(size=50),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.5],[0.5])])\n",
    "    image_tensor = transformations(img)[:3,:,:].unsqueeze(0)\n",
    "\n",
    "    # move model inputs to cuda, if GPU available\n",
    "    if use_cuda:\n",
    "        image_tensor = image_tensor.cuda()\n",
    "\n",
    "    # get sample outputs\n",
    "    output = infer_model(image_tensor)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "\n",
    "    pred = np.squeeze(preds_tensor.numpy()[0]) if not use_cuda else np.squeeze(preds_tensor.cpu().numpy()[0])\n",
    "\n",
    "    return dict_labels[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = cv2.VideoCapture(0)\n",
    "rval, frame = vc.read()\n",
    "old_text = ''\n",
    "pred_text = ''\n",
    "count_frames = 0\n",
    "total_str = ''\n",
    "flag = False\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if frame is not None: \n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame = cv2.resize( frame, (400,400) )\n",
    "        \n",
    "        cv2.rectangle(frame, (300,300), (100,100), (0,255,0), 2)\n",
    "        \n",
    "        crop_img = frame[100:300, 100:300]\n",
    "        grey = cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        thresh = cv2.threshold(grey,210,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)[1]\n",
    "      \n",
    "        \n",
    "        blackboard = np.zeros(frame.shape, dtype=np.uint8)\n",
    "        cv2.putText(blackboard, \"Predicted text - \", (30, 40), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 0))\n",
    "        if count_frames > 20 and pred_text != \"\":\n",
    "            total_str += pred_text\n",
    "            count_frames = 0\n",
    "            \n",
    "        if flag == True:\n",
    "            old_text = pred_text\n",
    "            pred_text = predictor(thresh)\n",
    "        \n",
    "            if old_text == pred_text:\n",
    "                count_frames += 1\n",
    "            else:\n",
    "                count_frames = 0\n",
    "            cv2.putText(blackboard, total_str, (30, 80), cv2.FONT_HERSHEY_TRIPLEX, 1, (255, 255, 127))\n",
    "        res = np.hstack((frame, blackboard))\n",
    "        \n",
    "        cv2.imshow(\"image\", res)\n",
    "        cv2.imshow(\"hand\", thresh)\n",
    "        \n",
    "    rval, frame = vc.read()\n",
    "    keypress = cv2.waitKey(1)\n",
    "    if keypress == ord('c'):\n",
    "        flag = True\n",
    "    if keypress == ord('q'):\n",
    "        break\n",
    "\n",
    "vc.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
